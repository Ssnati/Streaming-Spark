# Web Scraper + Spark Streaming - FincaraÃ­z

Proyecto completo que demuestra web scraping y procesamiento en tiempo real con Spark Streaming para propiedades inmobiliarias.

## ğŸš€ CaracterÃ­sticas

### 1. Web Scraping BÃ¡sico
- ExtracciÃ³n de propiedades desde FincaraÃ­z Colombia
- PaginaciÃ³n automÃ¡tica (5 pÃ¡ginas por defecto)
- ExtracciÃ³n inteligente de barrios conservando artÃ­culos (ej: "La Calleja")
- Datos guardados en CSV con timestamps

### 2. Spark Streaming
- Procesamiento de datos en tiempo real con Spark 3.4.1
- AnÃ¡lisis por lotes (batch) y en tiempo real (streaming)
- CÃ¡lculo de mÃ©tricas de precios por barrio
- VisualizaciÃ³n de tendencias de precios

## ğŸ§ InstalaciÃ³n en Linux

### 1. Requisitos previos

```bash
# Actualizar el sistema
sudo apt update && sudo apt upgrade -y

# Instalar dependencias del sistema
sudo apt install -y \
    python3-pip \
    python3-venv \
    openjdk-11-jdk \
    git \
    libxml2-dev \
    libxslt1-dev \
    python3-lxml

# Verificar instalaciÃ³n de Java (requerido para Spark)
java -version  # Debe mostrar versiÃ³n 11 o superior
```

### 2. ConfiguraciÃ³n del entorno virtual

```bash
# Crear y activar entorno virtual
python3 -m venv venv
source venv/bin/activate

# Actualzar pip
pip install --upgrade pip
```

### 3. InstalaciÃ³n de dependencias

#### OpciÃ³n 1: Dependencias mÃ­nimas (recomendado para producciÃ³n)
```bash
pip install -r requirements-min.txt
```

#### OpciÃ³n 2: Todas las dependencias (desarrollo)
```bash
pip install -r requirements.txt
```

### 4. ConfiguraciÃ³n de variables de entorno

Crear un archivo `.env` en la raÃ­z del proyecto:

```bash
# ConfiguraciÃ³n de Spark
SPARK_HOME=venv/lib/python3.10/site-packages/pyspark
PYSPARK_PYTHON=python3
PYSPARK_DRIVER_PYTHON=python3

# ConfiguraciÃ³n de la aplicaciÃ³n
LOG_LEVEL=INFO
DATA_DIR=./data
```

## ğŸš€ Uso

### 1. Web Scraping

```bash
# Scraping bÃ¡sico (5 pÃ¡ginas)
python web_scraper.py

# Especificar nÃºmero de pÃ¡ginas
python web_scraper.py --pages 3

# Solo primera pÃ¡gina
python web_scraper.py --single

# Usar URL personalizada
python web_scraper.py --url "https://www.fincaraiz.com.co/venta/casas/otra-ciudad/otro-departamento"
```

### 2. AnÃ¡lisis por lotes (Batch)

```bash
# Procesar datos estÃ¡ticos
python spark_streaming_analysis.py
```

### 3. AnÃ¡lisis en tiempo real (Streaming)

```bash
# Iniciar el procesamiento en tiempo real
python streaming_analysis.py
```

## ğŸ“Š Estructura de archivos

```
.
â”œâ”€â”€ files/
â”‚   â””â”€â”€ csv/                    # Archivos CSV de entrada/salida
â”‚       â””â”€â”€ propiedades_*.csv   # Datos de propiedades
â”œâ”€â”€ output/                     # Resultados de anÃ¡lisis
â”œâ”€â”€ scrapped_data/              # HTMLs descargados (debug)
â”œâ”€â”€ venv/                      # Entorno virtual
â”œâ”€â”€ .env                       # Variables de entorno
â”œâ”€â”€ requirements.txt           # Dependencias completas
â”œâ”€â”€ requirements-min.txt       # Dependencias mÃ­nimas
â”œâ”€â”€ web_scraper.py             # Script de scraping
â”œâ”€â”€ spark_streaming_analysis.py # AnÃ¡lisis por lotes
â””â”€â”€ streaming_analysis.py      # AnÃ¡lisis en tiempo real
```

## ğŸ“ Notas

- Los datos se guardan automÃ¡ticamente en `files/csv/` con timestamps
- Los anÃ¡lisis generan informes en la carpeta `output/`
- Se recomienda revisar los logs para depuraciÃ³n

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

#### Sistema Completo
```bash
cd spark
python streaming_coordinator.py
```

## ğŸ“Š AnÃ¡lisis de Spark Streaming

El sistema ejecuta 5 anÃ¡lisis en tiempo real:

1. **ğŸ“‹ Todas las propiedades** (cada 10s)
   - Muestra propiedades procesadas en tiempo real

2. **ğŸ“ EstadÃ­sticas por ubicaciÃ³n** (cada 15s)
   - Precio promedio, mÃ¡ximo, mÃ­nimo por zona
   - NÃºmero total de propiedades

3. **ğŸ  AnÃ¡lisis por tipo** (cada 20s)
   - Casa vs Apartamento vs Duplex
   - Precio y Ã¡rea promedio por tipo

4. **ğŸ’° Mejor relaciÃ³n calidad-precio** (cada 25s)
   - Propiedades ordenadas por precio/mÂ²
   - Detecta las mejores ofertas

5. **â­ Alertas propiedades premium** (cada 30s)
   - Propiedades > $500M COP
   - CaracterÃ­sticas de lujo

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ Spark.py                 # Script principal de scraping
â”œâ”€â”€ requirements.txt         # Todas las dependencias del entorno
â”œâ”€â”€ requirements-min.txt     # Dependencias mÃ­nimas del proyecto
â”œâ”€â”€ .gitignore              # Archivos a ignorar en Git
â”œâ”€â”€ README.md               # Este archivo
â”œâ”€â”€ propiedades.csv         # Datos extraÃ­dos (generado)
â”œâ”€â”€ barrios_tunja_completo.csv # Datos de barrios convertidos desde KML
â”œâ”€â”€ kml_to_csv_simple.py    # Conversor de KML a CSV
â””â”€â”€ data/                   # Carpeta de archivos de datos
    â””â”€â”€ html_response_*.html # Respuestas HTML (generadas)
```

## ğŸ® GuÃ­a RÃ¡pida

### Para probar todo el sistema (2 minutos):
```bash
# 1. Instalar dependencias
pip install -r requirements-min.txt

# 2. Ejecutar menÃº principal
python main.py

# 3. Seleccionar opciÃ³n 2 (Demo Spark Streaming)
# 4. Â¡Ver la magia! ğŸ‰
```

### Para scraping real de FincaraÃ­z:
```bash
# Desde el menÃº principal
python main.py
# Seleccionar opciÃ³n 1 (Web Scraping bÃ¡sico)
```

## ğŸ”§ Requisitos del Sistema

- **Python**: 3.7 o superior
- **Java**: 8 o 11 (para PySpark)
- **RAM**: MÃ­nimo 4GB recomendado
- **SO**: Windows/Linux/macOS

## ğŸ“ˆ Datos Generados

### CSV BÃ¡sico (`data/propiedades.csv`)
- title, price, location, rooms, bathrooms, area_m2, publisher, url

### Streaming JSON (formato enriquecido)
- Todos los campos anteriores +
- timestamp, property_type, amenities, condition, construction_year
- price_per_m2, floors, parking, description

## ğŸ¯ Casos de Uso Demostrados

1. **ETL en Tiempo Real**: ExtracciÃ³n, transformaciÃ³n y carga continua
2. **AnÃ¡lisis de Mercado**: EstadÃ­sticas de precios en vivo
3. **Alertas AutomÃ¡ticas**: DetecciÃ³n de propiedades premium
4. **Calidad de Datos**: Limpieza y validaciÃ³n en streaming
5. **Agregaciones Complejas**: MÃºltiples mÃ©tricas simultÃ¡neas

## ğŸš¨ Troubleshooting

### Error: "Java not found"
```bash
# Instalar Java 11
# Windows: Descargar desde Oracle/OpenJDK
# Linux: sudo apt install openjdk-11-jdk
# macOS: brew install openjdk@11
```

### Error: "No module named pyspark"
```bash
pip install pyspark==3.4.1
```

### Sin datos en streaming
```bash
# Desde el menÃº principal, seleccionar opciÃ³n 4
python main.py
```

## ğŸ“ Notas Importantes

- **Streaming Data**: Los archivos JSON se generan en `data/streaming_data/`
- **Rate Limiting**: 2 segundos entre requests para respetar el servidor
- **Memory**: Spark usa ~2GB RAM por defecto
- **Performance**: Mejor rendimiento con SSD
- **Estructura Modular**: CÃ³digo organizado en mÃ³dulos especÃ­ficos

## ğŸ“ Conceptos Demostrados

### Web Scraping
- **Beautiful Soup**: Parsing de HTML
- **Requests**: Manejo de HTTP
- **Rate Limiting**: Respeto al servidor
- **Error Handling**: Manejo de errores robusto

### Spark Streaming
- **Structured Streaming**: API moderna de Spark
- **Micro-batch Processing**: Procesamiento por lotes pequeÃ±os
- **Window Functions**: AnÃ¡lisis temporal
- **Multiple Sinks**: MÃºltiples outputs simultÃ¡neos
- **Schema Evolution**: Manejo de cambios de estructura

### Arquitectura de Software
- **Modularidad**: SeparaciÃ³n clara de responsabilidades
- **Escalabilidad**: FÃ¡cil agregar nuevas fuentes
- **Mantenibilidad**: CÃ³digo organizado y documentado

---

## ğŸ† Â¿Por quÃ© este proyecto es ideal para demostrar Spark Streaming?

1. **âœ… Datos Realistas**: Propiedades inmobiliarias reales
2. **âœ… Flujo Continuo**: Datos llegan gradualmente
3. **âœ… AnÃ¡lisis Variados**: 5 tipos diferentes de consultas
4. **âœ… Escalabilidad**: FÃ¡cil agregar mÃ¡s fuentes de datos
5. **âœ… VisualizaciÃ³n**: Resultados claros en consola
6. **âœ… PrÃ¡ctica Real**: Caso de uso empresarial tÃ­pico
7. **âœ… CÃ³digo Organizado**: Estructura profesional modular

Â¡Perfecto para demostrar las capacidades de Spark Streaming en un contexto real! ğŸš€

## ğŸ”§ Requisitos del Sistema

- **Python**: 3.7 o superior
- **Java**: 8 o 11 (para PySpark)
- **RAM**: MÃ­nimo 4GB recomendado
- **SO**: Windows/Linux/macOS

## ğŸ“ˆ Datos Generados

### CSV BÃ¡sico (`propiedades.csv`)
- title, price, location, rooms, bathrooms, area_m2, publisher, url

### Streaming JSON (formato enriquecido)
- Todos los campos anteriores +
- timestamp, property_type, amenities, condition, construction_year
- price_per_m2, floors, parking, description

## ğŸ¯ Casos de Uso Demostrados

1. **ETL en Tiempo Real**: ExtracciÃ³n, transformaciÃ³n y carga continua
2. **AnÃ¡lisis de Mercado**: EstadÃ­sticas de precios en vivo
3. **Alertas AutomÃ¡ticas**: DetecciÃ³n de propiedades premium
4. **Calidad de Datos**: Limpieza y validaciÃ³n en streaming
5. **Agregaciones Complejas**: MÃºltiples mÃ©tricas simultÃ¡neas

## ğŸš¨ Troubleshooting

### Error: "Java not found"
```bash
# Instalar Java 11
# Windows: Descargar desde Oracle/OpenJDK
# Linux: sudo apt install openjdk-11-jdk
# macOS: brew install openjdk@11
```

### Error: "No module named pyspark"
```bash
pip install pyspark==3.4.1
```

### Sin datos en streaming
```bash
# Verificar directorio
ls streaming_data/

# Regenerar datos
python synthetic_data_generator.py
```

## ğŸ“ Notas Importantes

- **Streaming Data**: Los archivos JSON se generan en `streaming_data/`
- **Rate Limiting**: 2 segundos entre requests para respetar el servidor
- **Memory**: Spark usa ~2GB RAM por defecto
- **Performance**: Mejor rendimiento con SSD

## ğŸ“ Conceptos Demostrados

- **Spark Streaming**: Procesamiento micro-batch
- **Structured Streaming**: API moderna de Spark
- **Window Functions**: AnÃ¡lisis temporal
- **Watermarking**: Manejo de datos tardÃ­os
- **Multiple Sinks**: MÃºltiples outputs simultÃ¡neos
- **Schema Evolution**: Manejo de cambios de estructura

---

## ğŸ† Â¿Por quÃ© este proyecto es ideal para mostrar Spark Streaming?

1. **âœ… Datos Realistas**: Propiedades inmobiliarias reales
2. **âœ… Flujo Continuo**: Datos llegan gradualmente
3. **âœ… AnÃ¡lisis Variados**: 5 tipos diferentes de consultas
4. **âœ… Escalabilidad**: FÃ¡cil agregar mÃ¡s fuentes de datos
5. **âœ… VisualizaciÃ³n**: Resultados claros en consola
6. **âœ… PrÃ¡ctica Real**: Caso de uso empresarial tÃ­pico

Â¡Perfecto para demostrar las capacidades de Spark Streaming! ğŸš€
